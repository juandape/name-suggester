#!/usr/bin/env node

import { execSync } from 'child_process';

async function setupOllama() {
  console.log('üîç Verificando instalaci√≥n de Ollama...');

  try {
    // Verificar si ollama est√° instalado
    execSync('which ollama', { stdio: 'ignore' });
    console.log('‚úÖ Ollama est√° instalado');

    // Verificar si el servicio est√° corriendo
    try {
      const response = await fetch('http://localhost:11434/api/tags');
      if (response.ok) {
        console.log('‚úÖ Servicio de Ollama est√° corriendo');

        // Verificar si hay modelos instalados
        const data = await response.json();
        if (data.models && data.models.length > 0) {
          console.log(
            '‚úÖ Modelos disponibles:',
            data.models.map((m) => m.name).join(', ')
          );
        } else {
          console.log(
            '‚ö†Ô∏è No hay modelos instalados. Descargando llama3.2:1b...'
          );
          execSync('ollama pull llama3.2:1b', { stdio: 'inherit' });
        }
      } else {
        console.log('‚ö†Ô∏è Ollama no est√° corriendo. Iniciando servicio...');
        execSync('brew services start ollama', { stdio: 'inherit' });
      }
    } catch (error) {
      console.log('‚ö†Ô∏è Ollama no est√° corriendo. Iniciando servicio...');
      execSync('brew services start ollama', { stdio: 'inherit' });
    }
  } catch (error) {
    console.log('‚ùå Ollama no est√° instalado');
    console.log('Para usar IA local gratuita, instala Ollama:');
    console.log('üí° Instrucciones: https://ollama.ai');
    console.log('');
    console.log('O usa OpenAI configurando OPENAI_API_KEY en tu archivo .env');
  }
}

setupOllama().catch(console.error);
